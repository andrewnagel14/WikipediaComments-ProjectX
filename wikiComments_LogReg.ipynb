{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On8Kwqe_4aI7",
        "outputId": "24c04524-bdaf-4c60-b15a-5a8eec2521e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "eng_stopwords = stopwords.words('english')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Function to clean data\n",
        "def clean_data(df):\n",
        "    # Replace new line with a space\n",
        "    df['clean'] = df['comment_text'].apply(lambda row: re.sub(\"\\n\", \" \", str(row)))\n",
        "    # Replace non-character with a \"\"\n",
        "    df['clean'] = df['clean'].apply(lambda comment: re.sub(\"[^A-Za-z\\' ]+\", \"\", comment))\n",
        "    # Remove stopwords from comment and make lowercase\n",
        "    df['clean'] = df['clean'].apply(\n",
        "        lambda comment: \" \".join([word.lower() for word in comment.split() if word not in eng_stopwords]))\n",
        "    print(\"Cleaning...\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "mGQTD3StrgVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Function to transform data in TF-IDF vectors\n",
        "def transform_tfidf(text):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vectorizer.fit_transform(text)\n",
        "    print(\"Fitting TFIDF Vectorizer...\")\n",
        "    return vectorizer\n"
      ],
      "metadata": {
        "id": "rsqaUTEirjuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### READ in xlsx file as data frame (columns: id, comment_text, toxic, severe_toxic, obscene, threat, insult, identity_hate)\n",
        "data = pd.read_excel('toxic_comment_dataset.xlsx')  \n",
        "\n",
        "### Drop user ID column because we don't need it\n",
        "data = data.drop(columns=['id'])\n",
        "\n",
        "# Preprocessing the data\n",
        "data_clean = clean_data(data)\n",
        "vectorizer_tfidf = transform_tfidf(data_clean['clean'].values)\n",
        "data_tfidf = vectorizer_tfidf.transform(data_clean['clean'].values)\n",
        "\n",
        "# Build logistic regression model\n",
        "lrcv = LogisticRegressionCV(cv=5, solver=\"liblinear\", random_state=0, max_iter=200)\n",
        "\n",
        "# Array for saving model output\n",
        "all_probabilities=[]\n",
        "confusion_matricies=[]\n",
        "\n",
        "# Evaluate model for each behaviour\n",
        "behaviour_type = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "for behaviour in behaviour_type:\n",
        "\n",
        "    print(\"Building logistic regression model for \"+behaviour)\n",
        "        \n",
        "    # Split dataset\n",
        "    train_x, valid_x, train_y, valid_y = train_test_split(data_tfidf, data_clean[behaviour], stratify=data_clean[behaviour])\n",
        "\n",
        "    # Train model\n",
        "    lrcv.fit(train_x, train_y)\n",
        "\n",
        "    # Evaluate validation scores \n",
        "    valid_score = lrcv.score(valid_x, valid_y)\n",
        "    print(\"Validation Score - \" + str(valid_score))\n",
        "\n",
        "    # Evalute confusion matrix for classification\n",
        "    test_pred = lrcv.predict(valid_x)\n",
        "    confusion_matricies.append(confusion_matrix(valid_y,test_pred))\n",
        "\n",
        "    # Evaluate model for probabilities\n",
        "    all_probabilities.append(lrcv.predict_proba(valid_x)[:,1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kRITiNxrluc",
        "outputId": "863ee667-41cd-4f79-d9b0-f61274845ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning...\n",
            "Fitting TFIDF Vectorizer...\n",
            "Building logistic regression model for toxic\n",
            "Validation Score - 0.9598175118441832\n",
            "Building logistic regression model for severe_toxic\n",
            "Validation Score - 0.9905246534479734\n",
            "Building logistic regression model for obscene\n",
            "Validation Score - 0.9782417968064573\n",
            "Building logistic regression model for threat\n",
            "Validation Score - 0.9972676910736219\n",
            "Building logistic regression model for insult\n",
            "Validation Score - 0.971348356854586\n",
            "Building logistic regression model for identity_hate\n",
            "Validation Score - 0.9919534755470885\n"
          ]
        }
      ]
    }
  ]
}